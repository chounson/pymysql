from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import pickle
import time
import re
from bs4 import BeautifulSoup
import pymongo

KEY='没事'

b=webdriver.Chrome()
wait=WebDriverWait(b,5)

def save_info(info):
    print(info)


def get_search():
    b.get('https://www.taobao.com')
    b.delete_all_cookies()
    with open('/home/zelin/ddd/cookies.dat','rb') as f:
        cookies=pickle.load(f)
    for cookie in cookies:
        b.add_cookie(cookie)
    input=wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'#q')))
    button=wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,'#J_TSearchForm > div.search-button > button')))
    input.send_keys(KEY)
    button.click()
    wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,'#mainsrp-pager > div > div > div > ul > li.item.active'),'1'))
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'#mainsrp-itemlist > div > div > div:nth-of-type(1) > div')))
    get_info(b.page_source)
    page=int(re.search(r'(\d+)',page).group(1))
    return page

def get_info(html):
    soup=BeautifulSoup(html,'html.parser')
    results=soup.select('#mainsrp-itemlist > div > div > div.items > div.item')
    for result in results:
        info={
            'price':result.select('.price strong')[0].get_text(),
            'name':result.select('.title > a')[0].get_text().strip(),
            'shop':result.select('.shop > a')[0].get_text(),
            'href':reuslt.select('.pic > a')[0].attrs['href']
        }
        save_info(info)

def get_next(page):
    input=wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'#mainsrp-pager > div > div > div > div.form > input')))
    button=wait.unti(EC.element_to_be_clickable((By.CSS_SELECTOR,'#mainsrp-pager > div > div > div > div.form > span.btn.J_Submit')))
    input.clear()
    input.send_keys(page)
    button.click()
    wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,'#mainsrp-pager > div > div > div > ul > li.item.active > span'), str(page)))
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'#mainsrp-itemlist > div > div > div:nth-of-type(1) > div')))
    get_info(b.page_source)


def main():
    page=get_search()
    for pg in range(2,page+1):
        time.sleep(1)
        print(pg)
        get_next(pg)
    time.sleep(3)
    b.close()

main()
